{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport re\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport gensim \nfrom gensim.models import word2vec\nfrom gensim.models import KeyedVectors\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\nword_vectors = KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin',binary=True)\ntokenizer = RegexpTokenizer(\"[a-zA-Z0-9]+\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"df_fake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\ndf_true = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_fake['Label'] = 1\ndf_true['Label'] = 0\ndf_total = pd.concat([df_true,df_fake],axis=0,ignore_index=True)","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_total['text'] = df_total['title'] + df_total['text']\ndf_total = df_total.drop(['title'], axis=1)","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    \n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('Reuters','',text)\n    return text","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_total['text'] = df_total['text'].apply(lambda x:clean_text(x))","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')\ndf_total['text'] = df_total['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def lemmatize_words(text):\n    wnl = nltk.stem.WordNetLemmatizer()\n    lem = ' '.join([wnl.lemmatize(word) for word in text.split()])    \n    return lem","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_total['text'] = df_total['text'].apply(lemmatize_words)","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(df_total.shape)","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(44898, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"def getStemmedReview(review):\n    #   review = review.replace(\"<br /><br />\",\" \")\n    #Tokenize\n    \n    tokens = tokenizer.tokenize(review)\n  #  new_tokens = [token for token in tokens if token not in sw]\n    new_tokens = [token for token in tokens if  token.isalpha()]\n    new_tokens = [token for token in new_tokens if len(token)>1]\n    return len(new_tokens)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"no_of_tokens = 0\nfor i in range(0,len(df_total)):\n    temp = getStemmedReview(df_total['text'][i])\n    no_of_tokens+=temp\nmean_tokens = no_of_tokens/len(df_total)","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(mean_tokens)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"235.26673793932915\n","output_type":"stream"}]},{"cell_type":"code","source":"x_ = df_total['text']\ny_ = df_total['Label']","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(x_.shape)\nprint(y_.shape)","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(44898,)\n(44898,)\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef getStemmedReview(review, mean_tokens):\n    #   review = review.replace(\"<br /><br />\",\" \")\n    #Tokenize\n    \n    tokens = tokenizer.tokenize(review)\n  #  new_tokens = [token for token in tokens if token not in sw]\n    new_tokens = [token for token in tokens if  token.isalpha()]\n    new_tokens = [token for token in new_tokens if len(token)>1]\n    if len(new_tokens) > mean_tokens:\n        new_tokens = new_tokens[0:mean_tokens]\n    else:\n        temp = mean_tokens - len(new_tokens)\n        for i in range(0,temp):\n            new_tokens.append(0)\n            \n#    cleaned_review = ' '.join(new_tokens)\n\n    arr = np.zeros((mean_tokens,300))\n    for i in range(0,mean_tokens):\n        try:\n            emb = word_vectors[new_tokens[i]]\n        except:\n            emb = [0]*300\n        \n        arr[i] = emb\n        #for j in range(0,300):\n        #    arr[i][j] = emb[j]\n            \n    return arr","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"mean_len = 20\nembedding_out = np.zeros((len(df_total), mean_len, 300))\n#tokens_out = []\nfor i in range(0,len(df_total)):\n    temp = getStemmedReview(df_total['text'][i], mean_len)\n    embedding_out[i] = temp\n  #  for j in range(0,10):\n   #     for k in range(0,300):\n    #        embedding_out[i][j][k] = temp[j][k]","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(embedding_out.shape)","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(44898, 20, 300)\n","output_type":"stream"}]},{"cell_type":"code","source":"embedding_out_2d = embedding_out.reshape(embedding_out.shape[0], embedding_out.shape[1]*embedding_out.shape[2])","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(embedding_out_2d.shape)","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"(44898, 6000)\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(embedding_out_2d, y_, test_size=0.20, random_state=8)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"(35918, 6000)\n(8980, 6000)\n(35918,)\n(8980,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclf_gnb = GaussianNB()\nclf_gnb.fit(x_train, y_train)\n","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"GaussianNB()"},"metadata":{}}]},{"cell_type":"code","source":"pred_gnb = clf_gnb.predict(x_test)","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_test, pred_gnb))\nprint(recall_score(y_test, pred_gnb))\nprint(precision_score(y_test, pred_gnb))\nprint(f1_score(y_test, pred_gnb))","metadata":{"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"0.9378619153674833\n0.9680152510061427\n0.9182238296162347\n0.9424623633738916\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}